{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Methods: Concepts\n",
    "\n",
    "## Bayes' Theorem (not covering it here as it is in my PhD and MSc theses)\n",
    "One good real-world application is in a *spam filter*.  **Naive Bayes'** can be used to develop a model that can discriminate normal (Ham) emails from garbage (Spam).  Lots of ways to improve it, but works fairly well in a basic sense.\n",
    "\n",
    "#### For more, check lecture 25-26\n",
    "\n",
    "## Spam Filter with Naive Bayes\n",
    "Supervised learning.\n",
    "\n",
    "### Steps\n",
    "\n",
    "* Read in emails and their classification of ham/spam **the bulk of the code**\n",
    "* Vectorize emails to numbers representing each word\n",
    "* Get a functional object that will perform *Multinomial Naive Bayes'* from *sklearn*\n",
    "* Fit vectorized emails\n",
    "* Check it worked with test cases\n",
    "\n",
    "### for more code and details, see NaiveBayes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## for more code and details, see NaiveBayes.ipynb\n",
    "\n",
    "import os\n",
    "import io\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "## *** Read in the emails and their classification ***\n",
    "def readFiles(path):\n",
    "    # NO CODE HERE, JUST READ IN FILES FROM A DIR \n",
    "    # AND RETURN: FULL PATH AND MESSAGES BODY\n",
    "    \n",
    "def dataFrameFromDirectory(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for filename, message in readFiles(path):\n",
    "        rows.append({'message': message, 'class': classification})\n",
    "        index.append(filename)\n",
    "\n",
    "    return DataFrame(rows, index=index)\n",
    "\n",
    "data = DataFrame({'message': [], 'class': []})\n",
    "data = data.append(dataFrameFromDirectory('spamdir', 'spam')) #not real file/dir, just ex\n",
    "data = data.append(dataFrameFromDirectory('hamdir','ham'))#not real file/dir, just ex\n",
    "## *** Done reading in data ***\n",
    "\n",
    "# vectorize email contents to numbers\n",
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(data['message'].values)\n",
    "\n",
    "# make multinomial Naive Bayes object/func\n",
    "classifier = MultinomialNB()\n",
    "targets = data['class'].values\n",
    "# fit vectorized emails \n",
    "classifier.fit(counts, targets)\n",
    "\n",
    "# Check it worked with obviouse test casese\n",
    "examples = ['Free Viagra now!!!', \"Hi Bob, how about a game of golf tomorrow?\"]\n",
    "example_counts = vectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "* Attempts to split data into K groups that are closest to K centroids.\n",
    "\n",
    "    * (1)Centroids are adjusted to the center of the points that were closest to it.\n",
    "\n",
    "    * (2)Points are then used to find which centroids they are closest to again.\n",
    "\n",
    "* repeat 1 & 2 until error or distance centroids move converges.\n",
    "\n",
    "### Caveats\n",
    "\n",
    "* choosing K\n",
    "    * try increasing K until you stop getting large reductions in $\\chi^2$\n",
    "    \n",
    "* use different randomly chosen initial centroids to avoid local minima\n",
    "\n",
    "* Still need to determine labels for clusters found.\n",
    "\n",
    "#### Example of its use can be found in KMeans.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy \n",
    "\n",
    "* A measure of a data set's disorder - how same or different it is.  \n",
    "* Classify data set into N classes.\n",
    "    * Entropy of 0, implies all data is the same class\n",
    "    * High entropy, implies there are many types of classes in the data\n",
    "\n",
    "### Computing Entropy\n",
    "\n",
    "* $H(s) = -p_1ln(p_1) -...-p_nln(p_n)$\n",
    "* $p_i$ represents portion of data with that class/label\n",
    "* casses where all data is or all data is not a particular class contribute zero to entropy.  So, non-zero only when portions of the data are in different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "* flowcharts to assist with classification choices\n",
    "* EX. A tree of resume contents organized by its relation to the chances of being hired.\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "#### Can use 'from sklearn import tree',  AND pandas to organize data going into the trees.  Graphviz can be used to visualize resulting trees.\n",
    "\n",
    "* Decision trees are very susceptible to overfitting\n",
    "    * construct many trees in a 'forest' and have them all 'vote' towards the outcome classification\n",
    "         * MUST randomly sample data used to make each tree!\n",
    "         * Also, randomize the attributes each tree is fitting.\n",
    "         \n",
    "##### steps\n",
    "\n",
    "* read in data with pandas\n",
    "* convert columns used to make decisions into ordinal numbers with a map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "\n",
    "input_file = \"PastHires.csv\"\n",
    "df = pd.read_csv(input_file, header = 0)\n",
    "\n",
    "d = {'Y': 1, 'N': 0}\n",
    "df['Hired'] = df['Hired'].map(d)\n",
    "d = {'BS': 0, 'MS': 1, 'PhD': 2}\n",
    "df['Level of Education'] = df['Level of Education'].map(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* push these into a 'features' list\n",
    "* get array of matching decisions from supervised portion for training\n",
    "* make decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(df.columns[:6])\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(features,decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use graphviz to display resulting tree\n",
    "\n",
    "##### upgrade to using a random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(features,decisions)\n",
    "\n",
    "#Predict employment of an employed 10-year veteran\n",
    "print clf.predict([[10, 1, 4, 0, 0, 0]])\n",
    "#...and an unemployed 10-year veteran\n",
    "print clf.predict([[10, 0, 4, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Multiple models work together to make a prediction\n",
    "* Ex. random forests.\n",
    "\n",
    "#### methods\n",
    "\n",
    "* Bagging (bootsrap aggregating): many models built by training on randomly-drawn subsets of data\n",
    "* Boosting: additional models added to help address data mis-classified by previous model\n",
    "* 'bucket of models': Train multiple models, then pick one that works best on test data\n",
    "* Stacking: run multiple models on same data, then combine output results\n",
    "\n",
    "## Advanced Ensemble Learning\n",
    "\n",
    "* Bayes Optimal Classifier (BOC)\n",
    "    * Theoretically the best - but almost always impractical\n",
    "* Bayesian Parameter Averaging\n",
    "    * Attempts ot make BOC practical.  Still susceptible to overfitting, often outperformed by simple bagging\n",
    "* Bayesian Model Combination\n",
    "    * Tries to fix all of these\n",
    "    * BUT, ends up about the same as finding best combination of models with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "* Works well for high-dimensional data (lots of features)\n",
    "* Solves for high-dimensional support vectors to help divide up the data\n",
    "* Applies a 'kernal trick' to represent data in higher dimensions in order to find the hyperplanes not initially apparent in the lower diimensions.\n",
    "    * This is computationally expensive, and why it is not as useful for low-D data.\n",
    "\n",
    "Ex. Identify types of iris flower by length and width of sepal.\n",
    "\n",
    "###### General:\n",
    "With a simple linear kernal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "C = 1.0 #error penalty.  1 is default.\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(features, classifications)\n",
    "\n",
    "#Check for another set of features\n",
    "svc.predict([[200000, 40]])  #output will be classification for those features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "## User-Based Collaborative Filtering\n",
    "\n",
    "* Build matrix of things each user bought/viewed/rated\n",
    "* Compute similarity scores between users\n",
    "* Find similar users\n",
    "* Recommend stuff similar users boughts... that current user hasn't seen yet.\n",
    "\n",
    "#### Caveats\n",
    "\n",
    "* People's likes change\n",
    "* Number of people commonly >> number of items.  Thus, needs lots of filtering.\n",
    "* People intentially fabricate fake users to boost/trash to their advantage \n",
    "    * Shilling attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-based Collaborative Filtering\n",
    "\n",
    "Resolves some of the problems that arise from using people's actions to make recommendations mentioned above.\n",
    "* less items than people, faster to compute.\n",
    "* harder for people/users to intervene \n",
    "\n",
    "#### Idea\n",
    "\n",
    "* Find all pairs of items bought/viewed/rated by same user\n",
    "* Measure similarity of the item's ratings/bought... for all users that bought/viewed both\n",
    "* sort by item\n",
    "* sort by similarity\n",
    "* Use a look-up table of results to make recommendtions to users\n",
    "\n",
    "#### Steps\n",
    "\n",
    "* import data with pandas\n",
    "* convert data to database with items, users and rating/frequency bought..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## see 'SimilarMovies.ipynb' for basics of finding similar movies\n",
    "## see 'ItemBasedCF.ipynb' for improved filtering and results\n",
    "\n",
    "ratings = pd.read_csv('ratingsData') # not a real file, just ex\n",
    "movies = movies = pd.read_csv('items')# not a real file, just ex\n",
    "userRatings = ratings.pivot_table(index=['user_id'],columns=['title'],values='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate correlation between rating/frequency bought with pandas\n",
    "* Clean out spurrious results.  THIS IS TRICKY, BUT THE MOST IMPORTANT PART TO MAKE SURE RECOMMENDATIONS ARE SUCCESSFUL TO PRODUCING SALES.  Will probably go through many rounds of cleaning input data, tweaking correlation function, and cleaning resulting correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate item correlations\n",
    "corrMatrix = userRatings.corr()\n",
    "# ex of simple cleaning\n",
    "corrMatrix = userRatings.corr(method='pearson', min_periods=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use cleaned correlations array(s) to make recommendations\n",
    "* Try grouping results to help find top matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group results and return top matches\n",
    "simCandidates = simCandidates.groupby(simCandidates.index).sum()\n",
    "simCandidates.sort_values(inplace = True, ascending = False)\n",
    "simCandidates.head(10)\n",
    "# filter out those current user has seen or bought\n",
    "filteredSims = simCandidates.drop(myRatings.index)\n",
    "filteredSims.head(10)\n",
    "\n",
    "## further filtering ideas for this example at bottom of ItemBasedCF.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours (KNN)\n",
    "\n",
    "Supervised learning\n",
    "\n",
    "* Similar to K Means Clustering\n",
    "* Classify new data points based on their distance from known data\n",
    "* Find the K nearest neighbord, based on this 'distance'\n",
    "* Allow all KNN to vote on classification\n",
    "\n",
    "#### example in KNN.ipynb\n",
    "\n",
    "### Steps\n",
    "\n",
    "* import data with pandas\n",
    "* Group data by features of interest\n",
    "* Convert those for making classifications from into a normalized form\n",
    "* Make a distance calculating function\n",
    "* Find KNN\n",
    "* Sort or something to give results back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "### for more real code, see KNN.ipynb \n",
    "\n",
    "# bring in the data\n",
    "ratings = pd.read_csv(\"data\")# not a real file, just ex\n",
    "# group by features of interst\n",
    "movieProperties = ratings.groupby('movie_id').agg({'rating': [np.size, np.mean]})\n",
    "# normalize features of interest for classification\n",
    "movieNumRatings = pd.DataFrame(movieProperties['rating']['size'])\n",
    "movieNormalizedNumRatings = movieNumRatings.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def ComputeDistance(a, b):\n",
    "    \"\"\" Function to comput distance between two items.\"\"\"\n",
    "    genresA = a[1]\n",
    "    genresB = b[1]\n",
    "    genreDistance = spatial.distance.cosine(genresA, genresB)\n",
    "    popularityA = a[2]\n",
    "    popularityB = b[2]\n",
    "    popularityDistance = abs(popularityA - popularityB)\n",
    "    return genreDistance + popularityDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def getNeighbors(movieID, K):\n",
    "    \"\"\" Get KNN and return sorted neighbors.\"\"\"\n",
    "    distances = []\n",
    "    for movie in movieDict:\n",
    "        if (movie != movieID):\n",
    "            dist = ComputeDistance(movieDict[movieID], movieDict[movie])\n",
    "            distances.append((movie, dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(K):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors\n",
    "\n",
    "## again, see KNN.ipynb to see how the results can be \n",
    "## displayed to see how it went"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis (PCA)\n",
    "\n",
    "When data has too many dimensions, extract sets of basis data that can be combined to re-produce the high-D data sufficiently.  In another way: find a way to represent the data with minimal dimensions that sufficiently preserves its variance.\n",
    "\n",
    "### very useful in image compression and face recognition\n",
    "\n",
    "* Commonly implementation is Single Value Decomposition (SVD)\n",
    "\n",
    "Ex. Identify types of iris flower by length and width of sepal.  Data comes with scikit-learn.\n",
    "* With PCA 4 length & width of petals & sepal (4D) -> 2D\n",
    "\n",
    "#### See PCA.ipynb for all code\n",
    "\n",
    "### steps\n",
    "\n",
    "* Import data\n",
    "* Apply PCA\n",
    "* Check how much variance was captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import pylab as pl\n",
    "from itertools import cycle\n",
    "\n",
    "# load data\n",
    "iris = load_iris()\n",
    "#numSamples, numFeatures = iris.data.shape\n",
    "\n",
    "# apply PCA\n",
    "X = iris.data\n",
    "pca = PCA(n_components=2, whiten=True).fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print pca.components_\n",
    "#check remaining variance\n",
    "print pca.explained_variance_ratio_\n",
    "print sum(pca.explained_variance_ratio_)  #1.0 would implies 100% variance kept\n",
    "\n",
    "## see rest of PCA.ipynb to see how to plot resuls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Warehousing\n",
    "\n",
    "## ETL: Extract, Transform, Load\n",
    "\n",
    "The more 'traditional' approach.\n",
    "* raw data from operational systems periodically *extracted*\n",
    "* raw data is *transformed* into a required schema\n",
    "* transformed data is *loaded* into warehouse\n",
    "\n",
    "* BUT, step 2, *transform* can be a big problem with \"big data\"\n",
    "\n",
    "## ELT: Extract, Load, Transform\n",
    "\n",
    "Push intensive transformation step to the end where it can be better optimized.  This approach is now much more scalable than ETL.\n",
    "\n",
    "* Extract raw data as before\n",
    "* load it in to datawarehouse raw\n",
    "* let cluster (Hadoop) process and manage data in-place\n",
    "* Query reduced data with new methods such as NoSQL, Spark or MapReduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "One example is Pac-Man.\n",
    "### Idea:\n",
    "* agent 'explores' space\n",
    "* allow agent to learn values of different state changes in different conditions\n",
    "* state & choice values then used to make informed future decisions\n",
    "\n",
    "## Q-Learning\n",
    "Implementation of reinforcement learning.\n",
    "* have:\n",
    "    * set of environmental states **s**\n",
    "    * set of possible actions **a** for each state\n",
    "    * value of state/action **Q**\n",
    "\n",
    "* Start all Q's at 0\n",
    "* explore\n",
    "* bad things -> reduce Q for that state/action\n",
    "* good things -> increase Q\n",
    "\n",
    "### The exploration problem\n",
    "Use Bayes theorem to include intelligent randomness into exploration to increase the learning efficiency.  Thus, a **Markov Decision Process (MDP)**\n",
    "\n",
    "Use This in tandem with Q-learning to build up a list of all possible states and the reward values (Q values) for every available action in that state.  Can be considered to implement *Dynamic Programming* or *Memoization* in some cases or terms.\n",
    "\n",
    "## For more details, code and learning problems based on Pac-Man:\n",
    "[Pac-Man: Reinforcement Learning](https://inst.eecs.berkeley.edu/~cs188/sp12/projects/reinforcement/reinforcement.html)\n",
    "\n",
    "[Cat & Mouse Problem](https://github.com/studywolf/blog/tree/master/RL/Cat%20vs%20Mouse%20exploration)\n",
    "\n",
    "[Markov Decision Process (MDP) toolbox for Python](http://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Real-World Data\n",
    "\n",
    "Lectures 48-53 based on issues of applying course fundamentals to real world data.\n",
    "\n",
    "## Apache Spark: Machine Learning on Big Data\n",
    "\n",
    "Using **MLLib** to esentially do things like K-Means Clustering, Decision Trees... reviewed in pure Python before, but in a way that could be ran locally OR on a **Hadoop** cluster with **Amazon Web Services (AWS)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
