{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic notes of the important points made during each of the lectures for the [Coursera Machine Learning class taught by Andrew Ng, of Stanford](https://www.coursera.org/learn/machine-learning).\n",
    "\n",
    "The next step is to put these lessons into code in another notebook.\n",
    "___\n",
    "___\n",
    "## First: Other courses I have taken or were recommended to me\n",
    "* [MIT open course videos on YouTube](https://www.youtube.com/watch?v=HtSuA80QTyo&list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb)\n",
    "* [Data Science and Machine Learning course using Python](https://www.udemy.com/data-science-and-machine-learning-with-python-hands-on)\n",
    "    * I have summarized the key poits from this course in *Basic_DS_Udemy_notes.ipynb*\n",
    "* [Taming big data with Apache SPark - Hand on!](https://www.udemy.com/taming-big-data-with-apache-spark-hands-on)\n",
    "* [Python for Data Structures, Algorithms, and Interviews!](https://www.udemy.com/python-for-data-structures-algorithms-and-interviews).  \n",
    "    * Great for preparing for CS based interviews and an intense review of practical parts of 1st and 2nd year CS in general.\n",
    "* Data Science by Bill Howe (U-Washington)\n",
    "* *data scientist tutorial*: reviews simple application of scikit-learn\n",
    "* A big data course I have the vids for, but can't find proper title or instructor of... odd\n",
    "\n",
    "___\n",
    "___\n",
    "\n",
    "# Notes for Andrew Ng, Machine Learning class\n",
    "___\n",
    "___\n",
    "\n",
    "# Machine Learning: \n",
    "\n",
    "- Arthur Samuel (1959). Machine Learning: Field of\n",
    "study that gives computers the ability to learn\n",
    "without being explicitly programmed.\n",
    "\n",
    "- Tom Mitchell (1998) Well-posed Learning\n",
    "Problem: A computer program is said to learn\n",
    "from experience E with respect to some task T\n",
    "and some performance measure P, if its\n",
    "performance on T, as measured by P, improves\n",
    "with experience E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Machine Learning Algorithms:\n",
    "\n",
    "- Supervised \n",
    "- insupervised\n",
    "\n",
    "Others:\n",
    "- Reinforcement Learning, recommender systems..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised\n",
    "\n",
    "1. Hypothesis function $h(x)$ maps input feature values to output label/target values\n",
    "2. Cost function $J(h(x))$, needs to be minimized with some algorithm to determine the best set of fitting model parameters for the given data.\n",
    "3. Testing of the resulting best-fit hypothesis function needs to be performed to determine robustness/generalization and accuracy.\n",
    "\n",
    "### Regression: Used to predict continuous valued output\n",
    "\n",
    "* regression algorithms include:\n",
    "    * gradient decent \n",
    "\n",
    "Non-continuous label values -> classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised\n",
    "\n",
    "Algorithm naturally discovers different classes.\n",
    "\n",
    "Ex. Coctail party problem: using multiple microphones to remove background sounds (music, general chatter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient decent\n",
    "[Stochastic Gradient Decent sklearn](http://scikit-learn.org/stable/modules/sgd.html)\n",
    "\n",
    "- learning rate ($\\alpha$): \n",
    "    * Too big and it is likely to overshoot the local minima, and maybe even diverge \n",
    "    * Too small and it will take a long time to converge.  \n",
    "    \n",
    "Plotting J vs #iters will help visualize/determine which case you are in.  But, once chosen, one expects the slope to get more shallow the closer to the minima each iteration, thus the resulting step sizes decrease even with a fixed $\\alpha$.\n",
    "\n",
    "#### Notes:\n",
    "- Must make sure to update each model parameter simultaneously for each iteration to avoid skewing the direction the cost function takes you in.\n",
    "\n",
    "- Caution to starting position, can lead to local rather than global minima, thus need to try starting and many different starting positions in the parameter space.\n",
    "\n",
    "- 'batch' gradient decent: each step uses ALL the training examples.\n",
    "\n",
    "- Caution in cases where there is more features than training examples.\n",
    "\n",
    "## Feature scaling\n",
    "\n",
    "[sklearn min/max scaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "\n",
    "##### Idea: \n",
    "make sure features are on similar scales.  Should any features be 10's - 10000's of times greater in value or range than others, it can dramitcally decrease the speed/effectiveness of gradient decent.\n",
    "\n",
    "##### Options: \n",
    "\n",
    "- divide each feature value by its range.  ie. new = orig/(max-min)\n",
    "- Mean normilization: new=(orig-mean)/(max-min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient decent vs Normal equation\n",
    "\n",
    "### Gradient decent\n",
    "- need to choose alpha\n",
    "- need many iterations\n",
    "- works well even when there are many features being fit\n",
    "\n",
    "### Normal Equation\n",
    "- no need to choose alpha\n",
    "- don't need to iterate\n",
    "- need to compute an nxn matrix -> can be slow if many features (over ~ 10000)\n",
    "\n",
    "[Python implementation on StackOverflow](http://stackoverflow.com/questions/34170618/normal-equation-and-numpy-least-squares-solve-methods-difference-in-regress)\n",
    "\n",
    "[Example from course site](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classification\n",
    "\n",
    "Applying standard linear regression can lead to incorrect threshold classifiers -> use Logistic regression.\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "Use a [sigmoid/logistic function](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), valid between 0-1.  Solves for likelihood/probability of label=1 for each input example.\n",
    "\n",
    "Non-linear decision boundaries can be found with higher order polynomials for the hypothesis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified cost function allowing for only 1 or 0 label/target values, enables use of gradient decent or normal equation fitting.  Specific equations for the cost function and implementation into gradient decent discussed in mid-late slides of Chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithms\n",
    "- [Conjugate gradient](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_cg.html)\n",
    "- BFGS\n",
    "- [L-BFGS](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_l_bfgs_b.html)\n",
    "\n",
    "#### Advantages:\n",
    "* no need to pick alpha\n",
    "* often faster than standard Gradient decent\n",
    "* commonly included as options in high level fitting toolboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "\n",
    "One-vs-all (one-vs-rest): train for each classifier on its own vs all others, and repeat sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Overfitting (high variance) vs Underfitting (high bias)\n",
    "* **overfitting**: Too many features causes fitting to training set data to be generalized.  ie. fit has a high variance\n",
    "* **underfitting**: Too few features gives too simple of a fit that will work in many cases, but never gives a good fit.  ie. fit is biased by the assumed low number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions to overfitting:\n",
    "- reduce number of features\n",
    "- regularize the data: add a strong constant weight to reduce the magnitude of parameter values.  Then only important features get big enough values to impact resulting fits.\n",
    "\n",
    "Note: caution on not making weighting/regularization parameter ($\\lambda$) too small/ too big.  Too small still leads to over fitting, too big leads to flat fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "\n",
    "\n",
    "\n",
    "# *~ partly SKIPPING NEURAL NETWORKS (Chapters 8 & 9) FOR NOW~*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression or multivariate regression can become nearly impossible when the number of features becomes very large and the problem is non-linear.\n",
    "\n",
    "Moving to a Neural Network ML is a good way to solve non-linear models with many features\n",
    "\n",
    "each layer is made of neurons that all make simple Sigmoid type hypothesis based on their inputs.  Their outputs are termed \"actions\" that feed the next layer.  This way, a single set of direct inputs from the data can be converted into increasingly more complicated non-linear inputs to deeper layers that can make complex decisions.\n",
    "\n",
    "During 'Forward propagation' we go from inputs, through the layers, to the final output, calculating actions along the way.  \n",
    "\n",
    "During 'backward propagation', the errors caused by the input actions to lower layers are calculated and used to adjust the neurons decisions in a form of training.\n",
    "\n",
    "___\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing performance and debugging ML \n",
    "\n",
    "Split training data into 'training' and 'cross validation' sets.\n",
    "\n",
    "### Plot error $J$ vs degree of polynomial.\n",
    "- both $J_{train}$ and $J_{cv}$ are high and of similar values -> high bias and you need to add more features\n",
    "- $J_{cv}$ >> $J_{train}$ -> high variance, reduce the number of features or implement regularization\n",
    "\n",
    "### Plotting regularization parameter (lambda) vs error (J)\n",
    "- $\\lambda$ small, $J_{train}$ low, $J_{cv}$ high -> high variance, increase $\\lambda$\n",
    "- $\\lambda$ high, $J_{train}$ high, $J_{cv}$ hight -> high bias, reduce $\\lambda$\n",
    "\n",
    "## Learning curves\n",
    "\n",
    "### plot training set size (number of examples) (m) vs error (J)\n",
    "- $J_{train}$ low, $J_{cv}$ high -> increase size of training set\n",
    "- $J_{cv}$ and $J_{train}$ ~ same, and both high -> high bias, don't need more data, need to decrease $\\lambda$ or increase number of features.\n",
    "\n",
    "## debugging 'what to do next' summary\n",
    "\n",
    "- Get more training examples -> fixes high variance\t\n",
    "- Try smaller sets of features -> fixes high variance\t\n",
    "- Try getting additional features-> fixes high bias\t\n",
    "- Try adding polynomial features ->\tfixes high bias\t\n",
    "- Try decreasing -> fixes high bias\t\n",
    "- Try increasing ->\tfixes high variance\t\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
