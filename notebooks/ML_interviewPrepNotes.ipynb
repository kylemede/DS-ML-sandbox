{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently, this is just a place to store notes in prep for upcoming interviews with a few different firms doing various AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___\n",
    "# Topics I have covered in my online courses so far:\n",
    "\n",
    "#### Big Data/Distributed Computing:\n",
    "- Spark (Spark: MLlib, SQL, RDDs, DataFrames)\n",
    "- Using AWS Elastic MapReduce\n",
    "- Using Hadoop YARN\n",
    "\n",
    "#### Deep Learning:\n",
    "- intro to TensorFlow\n",
    "- review of background and multiple types of Neural Networks\n",
    "\n",
    "#### ML/DS topics:\n",
    "- Regression analysis\n",
    "- K-Means Clustering\n",
    "- Principal Component Analysis\n",
    "- Train/Test and cross validation\n",
    "- Bayesian Methods\n",
    "- Decision Trees and Random Forests\n",
    "- Multivariate Regression\n",
    "- Multi-Level Models\n",
    "- SVM Support Vector Machines\n",
    "- Reinforcement Learning\n",
    "- Collaborative Filtering\n",
    "- K-Nearest Neighbor\n",
    "- Bias/Variance Tradeoff\n",
    "- Ensemble Learning\n",
    "- Term Frequency / Inverse Document Frequency\n",
    "- Experimental Design and A/B Tests\n",
    "\n",
    "___\n",
    "\n",
    "## Courses taken so far:\n",
    "\n",
    "#### Udemy:\n",
    "- Data science and machine learning with Python - hands on! \n",
    "- Taming big data with Apache Spark and Python - hands on!\n",
    "- Python for Data Science and Machine Learning bootcamp (*in progress*)\n",
    "- Python for data structures, algorithms, and interviews!\n",
    "- Mastering Python\n",
    "\n",
    "#### Others:\n",
    "- Machine Learning - Andrew Ng (Coursera/Stanford) (*in progress*)\n",
    "- Data Science, by Bill Howe (Coursera/U-Washington) (*in progress*)\n",
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks are modeled after biological neural networks and attempt to allow computers to learn in a similar manner to humans through reinforcment learning.\n",
    "\n",
    "Neural networks attemp to solve problems that would normally be easy for humans, but hard for computers.\n",
    "\n",
    "### Use cases:\n",
    "- Pattern Recognition\n",
    "- Time series predictions\n",
    "- Signal processing\n",
    "- Anomaly detection\n",
    "- control\n",
    "\n",
    "\n",
    "\n",
    "## [Convolution Neural Networks (CNN)](http://deeplearning.net/tutorial/lenet.html)\n",
    "\n",
    "Inspired by Multilayered Perceptron (MLP) and the animal visual cortex.\n",
    "[\"Designed to use minimal amounts of preprocessing. They have wide applications in image and video recognition, recommender systems and natural language processing\"](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n",
    "\n",
    "[Multilayered Perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron) \"a type of feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs.  An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network. MLP is a modification of the standard linear perceptron and can distinguish data that are not linearly separable.\" **from Wikipedia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A **feedforward neural network** is an artificial neural network wherein connections between the units do not form a cycle. As such, it is different from recurrent neural networks.\" [wiki](https://en.wikipedia.org/wiki/Feedforward_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"***Backpropagation***, an abbreviation for \"backward propagation of errors\", is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network, so that the gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the loss function.\n",
    "\n",
    "Backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient. It is therefore usually considered to be a **supervised learning** method, although it is also used in some unsupervised networks such as autoencoders. It is a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. Backpropagation requires that the activation function used by the artificial neurons (or \"nodes\") be differentiable.\" [wiki](https://en.wikipedia.org/wiki/Backpropagation)\n",
    "\n",
    "\n",
    "# Radial Basis Function\n",
    "\n",
    "\"In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control.\" [wiki](https://en.wikipedia.org/wiki/Radial_basis_function_network)\n",
    "\n",
    "In Python you could use [neupy](https://pypi.python.org/pypi/neupy/0.1.0)  or [scipy.interpolate.Rbf](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.interpolate.Rbf.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "#### Can use 'from sklearn import tree',  AND pandas to organize data going into the trees.  Graphviz can be used to visualize resulting trees.\n",
    "\n",
    "* Decision trees are very susceptible to overfitting\n",
    "    * construct many trees in a 'forest' and have them all 'vote' towards the outcome classification\n",
    "         * MUST randomly sample data used to make each tree!\n",
    "         * Also, randomize the attributes each tree is fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Methods: Concepts\n",
    "\n",
    "## Bayes' Theorem (not covering it here as it is in my PhD and MSc theses)\n",
    "One good real-world application is in a *spam filter*.  **Naive Bayes'** can be used to develop a model that can discriminate normal (Ham) emails from garbage (Spam).  Lots of ways to improve it, but works fairly well in a basic sense.\n",
    "\n",
    "## Spam Classifier/Filter with Naive Bayes\n",
    "Supervised learning.\n",
    "\n",
    "### Steps\n",
    "\n",
    "* Read in emails and their classification of ham/spam **the bulk of the code**\n",
    "* Vectorize emails to numbers representing each word\n",
    "* Get a functional object that will perform *Multinomial Naive Bayes'* from *sklearn*\n",
    "* Fit vectorized emails\n",
    "* Check it worked with test cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing data\n",
    "\n",
    "Visualization of the data can enable one to discover much more than standardized statistics like *mean*, *median* or *mode*...  an [article highlighting this](http://www.astrobiased.com/2015/01/20/eda-and-graphics/) for the Anscombe’s Quartet problem.\n",
    "\n",
    "* An interesting option, is a library built ontop of matplotlib [seaborn](https://seaborn.github.io/index.html)\n",
    "* An entire article discussing major 7 available Python based tools [here](https://www.dataquest.io/blog/python-data-visualization-libraries/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "___\n",
    "___\n",
    "# Common models and tools in ML/AI:\n",
    "___\n",
    "___\n",
    "\n",
    "## Neural Network Models:\n",
    "___\n",
    "### Types:\n",
    "\n",
    "#### Feedforward (acyclic graphs)\n",
    "* Autoencoders\n",
    "* Denoising autoencoders\n",
    "* Restricted Boltzmann machines (stacked, they form deep-belief networks)\n",
    "\n",
    "#### Convolutional\n",
    "Deep convolutional networks are SOTA for images. There are many well known architectures, including AlexNet and VGGNet.\n",
    "\n",
    "Convolutional networks usually involved a combination of convolutional layers as well as subsampling and fully connected feedforward layers. \n",
    "\n",
    "#### Recurrent\n",
    "These handle time series data especially well. They can be combined with convolutional networks to generate captions for images. \n",
    "\n",
    "#### Recursive\n",
    "These handle natural language especially well\n",
    "___\n",
    "\n",
    "- MLP Multi-layer perceptron\n",
    "- CNN Convolutional Neural Network\n",
    "- RNN Recurrent Neural Network\n",
    "- RNN Recursive Neural Network\n",
    "- LSTM Long Short Term Memory\n",
    "- FRN Fully recurrent network\n",
    "- HN Hopfield network\n",
    "- EN Elman network\n",
    "- JN Jordan network\n",
    "- ESN Echo state network\n",
    "- BRNN Bi-directional RNN\n",
    "___\n",
    "\n",
    "## Stochastic gradient descent\n",
    "\n",
    "- Momentum SGD\n",
    "- AdaGrad\n",
    "- RMSprop\n",
    "- AdaDelta\n",
    "- Adam\n",
    "- Nestrov’s Accelerated gradient descent\n",
    "- Grave’s RMS prop\n",
    "\n",
    "___\n",
    "\n",
    "## Activation function\n",
    "\n",
    "Outputs of perceptrons/neurons/nodes generated by passing weighted inputs through an 'activation function'.\n",
    "- Relu     (simple rectifier.  returns max(x,0))\n",
    "- Sigmoid\n",
    "- Soft Max\n",
    "- Max Out\n",
    "- Tanh\n",
    "- Identity\n",
    "- Leaky ReLU\n",
    "- Clipped_RelU\n",
    "- Exponential Linear Unit\n",
    "- Log Soft Max\n",
    "- Soft Plus\n",
    "- Parametric ReLU\n",
    "\n",
    "___\n",
    "\n",
    "## Pre-learning program\n",
    "\n",
    "Training to estimate best weights for inputs to nodes.\n",
    "\n",
    "- Denoising Auto-Encoder\n",
    "- Auto-encoder\n",
    "- Add the user code\n",
    "- Deep Boltzmann Machine\n",
    "- Restricted Boltzmann Machine Gibbs sampling\n",
    "- Restricted Boltzmann Machine Contrastive Divergence\n",
    "- Deep Belief Network\n",
    "- Gaussian unit\n",
    "- RELU unit\n",
    "\n",
    "___\n",
    "\n",
    "## Data Normalization\n",
    "- standardization\n",
    "- ZPC whitening\n",
    "- ZCA whitening\n",
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Solutions/Distributed computing notes\n",
    "\n",
    "## Spark\n",
    "- Alternative to tools like MapReduce.\n",
    "- More flexible and can work with other file systems: Cassandra, AWS S3...\n",
    "- Keeps most data in memory to be faster, BUT can RAM can overflow\n",
    "- MapReduce writes back to disk after each step, so slower...\n",
    "- MLLib provides powerful, easy to use ML & data mining tools\n",
    "- RDD Resilient Distributed Datasets (OLD way, **latest version uses Dataframes like those in SQL and Pandas**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
