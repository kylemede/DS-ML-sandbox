{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Convolution Neural Networks (CNN)](http://deeplearning.net/tutorial/lenet.html)\n",
    "\n",
    "Inspired by Multilayered Perceptron (MLP) and the animal visual cortex.\n",
    "[\"Designed to use minimal amounts of preprocessing. They have wide applications in image and video recognition, recommender systems and natural language processing\"](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n",
    "\n",
    "[Multilayered Perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron) \"a type of feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs.  An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network. MLP is a modification of the standard linear perceptron and can distinguish data that are not linearly separable.\" **from Wikipedia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A **feedforward neural network** is an artificial neural network wherein connections between the units do not form a cycle. As such, it is different from recurrent neural networks.\" [wiki](https://en.wikipedia.org/wiki/Feedforward_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"***Backpropagation***, an abbreviation for \"backward propagation of errors\", is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network, so that the gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the loss function.\n",
    "\n",
    "Backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient. It is therefore usually considered to be a **supervised learning** method, although it is also used in some unsupervised networks such as autoencoders. It is a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. Backpropagation requires that the activation function used by the artificial neurons (or \"nodes\") be differentiable.\" [wiki](https://en.wikipedia.org/wiki/Backpropagation)\n",
    "\n",
    "\n",
    "# Radial Basis Function\n",
    "\n",
    "\"In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control.\" [wiki](https://en.wikipedia.org/wiki/Radial_basis_function_network)\n",
    "\n",
    "In Python you could use [neupy](https://pypi.python.org/pypi/neupy/0.1.0)  or [scipy.interpolate.Rbf](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.interpolate.Rbf.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "#### Can use 'from sklearn import tree',  AND pandas to organize data going into the trees.  Graphviz can be used to visualize resulting trees.\n",
    "\n",
    "* Decision trees are very susceptible to overfitting\n",
    "    * construct many trees in a 'forest' and have them all 'vote' towards the outcome classification\n",
    "         * MUST randomly sample data used to make each tree!\n",
    "         * Also, randomize the attributes each tree is fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Methods: Concepts\n",
    "\n",
    "## Bayes' Theorem (not covering it here as it is in my PhD and MSc theses)\n",
    "One good real-world application is in a *spam filter*.  **Naive Bayes'** can be used to develop a model that can discriminate normal (Ham) emails from garbage (Spam).  Lots of ways to improve it, but works fairly well in a basic sense.\n",
    "\n",
    "#### For more, check lecture 25-26\n",
    "\n",
    "## Spam Classifier/Filter with Naive Bayes\n",
    "Supervised learning.\n",
    "\n",
    "### Steps\n",
    "\n",
    "* Read in emails and their classification of ham/spam **the bulk of the code**\n",
    "* Vectorize emails to numbers representing each word\n",
    "* Get a functional object that will perform *Multinomial Naive Bayes'* from *sklearn*\n",
    "* Fit vectorized emails\n",
    "* Check it worked with test cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing data\n",
    "\n",
    "* An interesting option, is a library built ontop of matplotlib [seaborn](https://seaborn.github.io/index.html)\n",
    "* An entire article discussing major 7 available Python based tools [here](https://www.dataquest.io/blog/python-data-visualization-libraries/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
